use serde::{Deserialize, Serialize};

pub const API_BASE_URL: &str = "http://127.0.0.1:3010";

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct HealthResponse {
    pub status: String,
    pub documents: Option<usize>,
    pub vectors: Option<usize>,
    pub index_path: Option<String>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct SearchResult {
    pub content: String,
    pub score: f32,
    pub document: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct SearchResponse {
    pub status: String,
    pub results: Vec<SearchResult>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct DocumentsResponse {
    pub status: String,
    pub documents: Vec<String>,
    pub count: usize,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RequestChartPoint {
    pub ts: i64,
    pub latency_ms: f64,
}

#[derive(Debug, Serialize, Deserialize, Clone, Default)]
#[serde(default)]
pub struct LatencyBreakdown {
    pub p50_ms: f64,
    pub p95_ms: f64,
    pub p99_ms: f64,
}

#[derive(Debug, Serialize, Deserialize, Clone, Default)]
#[serde(default)]
pub struct StatusBreakdown {
    pub success_rate: f64,
    pub client_error_rate: f64,
    pub server_error_rate: f64,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RequestsSnapshot {
    pub request_rate_rps: f64,
    pub latency_p95_ms: f64,
    pub error_rate_percent: f64,
    #[serde(default)]
    pub latency_breakdown: LatencyBreakdown,
    #[serde(default)]
    pub status_breakdown: StatusBreakdown,
    pub points: Vec<RequestChartPoint>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RerankRequest {
    pub query: String,
    pub candidates: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SummarizeRequest {
    pub query: String,
    pub candidates: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct IndexInfoResponse {
    pub index_in_ram: bool,
    pub mode: String,
    pub warning: Option<String>,
    pub total_documents: usize,
    pub total_vectors: usize,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ChunkingLoggingResponse {
    pub status: String,
    pub request_id: String,
    pub logging_enabled: bool,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ReindexAsyncResponse {
    pub status: String,
    pub job_id: String,
    pub request_id: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ReindexStatusResponse {
    pub status: String,
    pub job_id: String,
    pub started_at: Option<String>,
    pub completed_at: Option<String>,
    pub vectors_indexed: Option<usize>,
    pub mappings_indexed: Option<usize>,
    pub error: Option<String>,
    pub request_id: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ChunkerConfigSnapshot {
    pub target_size: usize,
    pub min_size: usize,
    pub max_size: usize,
    pub overlap: usize,
    pub semantic_similarity_threshold: f32,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ChunkCommitResponse {
    pub status: String,
    pub message: String,
    pub request_id: String,
    pub chunker_config: ChunkerConfigSnapshot,
    pub reindex_status: String,
    pub reindex_job_id: Option<String>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ChunkCommitRequest {
    pub target_size: usize,
    pub min_size: usize,
    pub max_size: usize,
    pub overlap: usize,
    pub semantic_similarity_threshold: Option<f32>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
#[serde(default)]
pub struct LlmConfig {
    pub temperature: f32,
    pub top_p: f32,
    pub top_k: usize,
    pub max_tokens: usize,
    pub repeat_penalty: f32,
    pub frequency_penalty: f32,
    pub presence_penalty: f32,
    pub stop_sequences: Vec<String>,
    pub seed: Option<i64>,
    pub min_p: f32,
    pub typical_p: f32,
    pub tfs_z: f32,
    pub mirostat: i32,
    pub mirostat_eta: f32,
    pub mirostat_tau: f32,
    pub repeat_last_n: usize,
}

impl Default for LlmConfig {
    fn default() -> Self {
        Self {
            temperature: 0.7,
            top_p: 0.95,
            top_k: 40,
            max_tokens: 1024,
            repeat_penalty: 1.1,
            frequency_penalty: 0.0,
            presence_penalty: 0.0,
            stop_sequences: Vec::new(),
            seed: None,
            min_p: 0.0,
            typical_p: 1.0,
            tfs_z: 1.0,
            mirostat: 0,
            mirostat_eta: 0.1,
            mirostat_tau: 5.0,
            repeat_last_n: 64,
        }
    }
}

/// Supported LLM inference backends
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq, Default)]
#[serde(rename_all = "snake_case")]
pub enum BackendType {
    #[default]
    Ollama,
    LlamaCpp,
    #[serde(rename = "openai")]
    OpenAi,
    Anthropic,
    Vllm,
    Custom,
}

impl BackendType {
    /// Returns true if this backend supports local hardware configuration
    pub fn supports_hardware_config(&self) -> bool {
        matches!(self, Self::LlamaCpp | Self::Vllm)
    }

    /// Returns true if this backend supports thread configuration
    pub fn supports_thread_config(&self) -> bool {
        matches!(self, Self::Ollama | Self::LlamaCpp)
    }

    /// Returns true if this backend supports GPU configuration (num_gpu)
    pub fn supports_gpu_config(&self) -> bool {
        matches!(self, Self::Ollama | Self::LlamaCpp | Self::Vllm)
    }

    /// Returns true if this backend supports GPU layer offloading (n_gpu_layers)
    pub fn supports_gpu_layers(&self) -> bool {
        matches!(self, Self::LlamaCpp | Self::Vllm)
    }

    /// Returns true if this backend supports RoPE configuration
    pub fn supports_rope_config(&self) -> bool {
        matches!(self, Self::LlamaCpp)
    }

    /// Returns true if this backend supports low_vram and f16_kv options
    pub fn supports_memory_options(&self) -> bool {
        matches!(self, Self::LlamaCpp)
    }

    /// Returns true if this is a cloud/API-based backend
    pub fn is_cloud_backend(&self) -> bool {
        matches!(self, Self::OpenAi | Self::Anthropic)
    }

    /// Human-readable label for the backend
    pub fn label(&self) -> &'static str {
        match self {
            Self::Ollama => "Ollama 0.12.6",
            Self::LlamaCpp => "llama.cpp",
            Self::OpenAi => "OpenAI",
            Self::Anthropic => "Anthropic",
            Self::Vllm => "vLLM",
            Self::Custom => "Custom",
        }
    }

    /// All available backend types
    pub fn all() -> Vec<BackendType> {
        vec![
            Self::Ollama,
            Self::LlamaCpp,
            Self::OpenAi,
            Self::Anthropic,
            Self::Vllm,
            Self::Custom,
        ]
    }

    /// Convert to API string representation
    pub fn to_api_string(&self) -> &'static str {
        match self {
            Self::Ollama => "ollama",
            Self::LlamaCpp => "llama_cpp",
            Self::OpenAi => "openai",
            Self::Anthropic => "anthropic",
            Self::Vllm => "vllm",
            Self::Custom => "custom",
        }
    }

    /// Parse from API string representation
    pub fn from_api_string(s: &str) -> Self {
        match s {
            "ollama" => Self::Ollama,
            "llama_cpp" => Self::LlamaCpp,
            "openai" => Self::OpenAi,
            "anthropic" => Self::Anthropic,
            "vllm" => Self::Vllm,
            "custom" => Self::Custom,
            _ => Self::Ollama,
        }
    }
}

impl std::fmt::Display for BackendType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.label())
    }
}

#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]
#[serde(default)]
pub struct HardwareConfig {
    pub backend_type: String,
    pub model: String,
    pub num_thread: usize,
    pub num_gpu: usize,
    pub gpu_layers: usize,
    pub main_gpu: usize,
    pub low_vram: bool,
    pub f16_kv: bool,
    pub rope_frequency_base: f32,
    pub rope_frequency_scale: f32,
    pub numa: bool,
    pub num_ctx: usize,
    pub num_batch: usize,
    pub logits_all: bool,
    pub vocab_only: bool,
    pub use_mmap: bool,
    pub use_mlock: bool,
}

impl Default for HardwareConfig {
    fn default() -> Self {
        Self {
            backend_type: "ollama".to_string(),
            model: String::new(),
            num_thread: 1,
            num_gpu: 0,
            gpu_layers: 0,
            main_gpu: 0,
            low_vram: false,
            f16_kv: true,
            rope_frequency_base: 10_000.0,
            rope_frequency_scale: 1.0,
            numa: false,
            num_ctx: 2048,
            num_batch: 512,
            logits_all: false,
            vocab_only: false,
            use_mmap: true,
            use_mlock: false,
        }
    }
}

impl HardwareConfig {
    /// Get the backend type as enum
    pub fn get_backend_type(&self) -> BackendType {
        BackendType::from_api_string(&self.backend_type)
    }

    /// Set the backend type from enum
    pub fn set_backend_type(&mut self, bt: BackendType) {
        self.backend_type = bt.to_api_string().to_string();
    }
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct LlmConfigResponse {
    pub status: String,
    pub message: String,
    pub request_id: String,
    pub config: LlmConfig,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct HardwareConfigResponse {
    pub status: String,
    pub message: String,
    pub request_id: String,
    pub config: HardwareConfig,
}

#[derive(Debug, Serialize, Deserialize, Clone, Default)]
pub struct ApiKeysRequest {
    #[serde(default)]
    pub openai_api_key: String,
    #[serde(default)]
    pub anthropic_api_key: String,
}

#[derive(Debug, Serialize, Deserialize, Clone, Default)]
pub struct ApiKeysResponse {
    pub status: String,
    pub message: String,
    pub request_id: String,
    pub has_openai_key: bool,
    pub has_anthropic_key: bool,
    pub openai_key_masked: String,
    pub anthropic_key_masked: String,
    pub openai_from_env: bool,
    pub anthropic_from_env: bool,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CacheLayerStats {
    pub enabled: bool,
    pub total_searches: u64,
    pub hits: u64,
    pub misses: u64,
    pub hit_rate: f64,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CacheL2Stats {
    pub enabled: bool,
    pub l1_hits: u64,
    pub l1_misses: u64,
    pub l2_hits: u64,
    pub l2_misses: u64,
    pub total_items: u64,
    pub hit_rate: f64,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CacheCountersSnapshot {
    pub hits_total: i64,
    pub misses_total: i64,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RedisSummary {
    pub enabled: bool,
    pub connected: bool,
    pub ttl_seconds: u64,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CacheInfoResponse {
    pub request_id: String,
    pub l1: CacheLayerStats,
    pub l2: CacheL2Stats,
    pub redis: RedisSummary,
    pub counters: CacheCountersSnapshot,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RouteDropStat {
    pub route: String,
    pub drops: i64,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RateLimitConfigSnapshot {
    pub enabled: bool,
    pub trust_proxy: bool,
    pub search_qps: f64,
    pub search_burst: f64,
    pub upload_qps: f64,
    pub upload_burst: f64,
    pub exempt_prefixes: Vec<String>,
    pub rules: Vec<serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RateLimiterState {
    pub enabled: bool,
    pub active_keys: usize,
    pub capacity: usize,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RateLimitInfoResponse {
    pub request_id: String,
    pub total_drops: i64,
    pub drops_by_route: Vec<RouteDropStat>,
    pub config: RateLimitConfigSnapshot,
    pub limiter_state: RateLimiterState,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct LogEntry {
    pub timestamp: Option<String>,
    pub level: Option<String>,
    pub target: Option<String>,
    pub message: Option<String>,
    pub raw: String,
    pub fields: Option<serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct LogsResponse {
    pub request_id: String,
    pub file: Option<String>,
    pub entries: Vec<LogEntry>,
    pub note: Option<String>,
}

#[derive(Clone, Debug, Deserialize, Serialize, PartialEq)]
pub struct GpuInfo {
    pub index: usize,
    pub name: String,
    pub vendor: String,
    pub backend: String,
    pub device_type: String,
}

#[derive(Clone, Debug, Deserialize, Serialize, PartialEq)]
pub struct SystemInfo {
    pub os: String,
    pub os_family: String,
    pub arch: String,
    pub physical_cores: usize,
    pub logical_cores: usize,
}

/// Model information returned by the models endpoint
#[derive(Clone, Debug, Deserialize, Serialize, PartialEq)]
pub struct ModelInfo {
    pub name: String,
    #[serde(default)]
    pub size: Option<u64>,
    #[serde(default)]
    pub modified_at: Option<String>,
    #[serde(default)]
    pub family: Option<String>,
}

impl ModelInfo {
    /// Format size in human-readable format (e.g., "3.8 GB")
    pub fn size_display(&self) -> String {
        match self.size {
            Some(bytes) => {
                let gb = bytes as f64 / 1_073_741_824.0;
                if gb >= 1.0 {
                    format!("{:.1} GB", gb)
                } else {
                    let mb = bytes as f64 / 1_048_576.0;
                    format!("{:.1} MB", mb)
                }
            }
            None => String::new(),
        }
    }
}

pub async fn fetch_physical_cores() -> Result<usize, String> {
    fetch_json::<usize>("/sys/cores").await
}

/// Fetch detailed GPU information
pub async fn fetch_gpus() -> Result<Vec<GpuInfo>, String> {
    fetch_json::<Vec<GpuInfo>>("/sys/gpus").await
}

/// Fetch simple GPU names list (backward compatible)
pub async fn fetch_gpu_names() -> Result<Vec<String>, String> {
    fetch_json::<Vec<String>>("/sys/gpu-names").await
}

/// Fetch system information including OS, architecture, and CPU cores
pub async fn fetch_system_info() -> Result<SystemInfo, String> {
    fetch_json::<SystemInfo>("/sys/info").await
}

/// Fetch available models for a given backend type
pub async fn fetch_models(backend: &str) -> Result<Vec<ModelInfo>, String> {
    let url = format!("/sys/models?backend={}", backend);
    fetch_json::<Vec<ModelInfo>>(&url).await
}

/// Check backend health
pub async fn health_check() -> Result<HealthResponse, String> {
    let url = format!("{}/monitoring/health", API_BASE_URL);

    gloo_net::http::Request::get(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

/// Search documents
pub async fn search(query: &str) -> Result<SearchResponse, String> {
    let url = format!("{}/search?q={}", API_BASE_URL, urlencoding::encode(query));

    gloo_net::http::Request::get(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

/// List all documents
pub async fn list_documents() -> Result<DocumentsResponse, String> {
    let url = format!("{}/documents", API_BASE_URL);

    gloo_net::http::Request::get(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

/// Delete a document
pub async fn delete_document(filename: &str) -> Result<serde_json::Value, String> {
    let url = format!("{}/documents/{}", API_BASE_URL, filename);

    gloo_net::http::Request::delete(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

/// Trigger reindexing
pub async fn reindex() -> Result<serde_json::Value, String> {
    let url = format!("{}/reindex", API_BASE_URL);

    gloo_net::http::Request::post(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

pub async fn reindex_async() -> Result<ReindexAsyncResponse, String> {
    let url = format!("{}/reindex/async", API_BASE_URL);

    gloo_net::http::Request::post(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

pub async fn commit_chunk_config(
    payload: &ChunkCommitRequest,
) -> Result<ChunkCommitResponse, String> {
    let url = format!("{}/config/chunk_size", API_BASE_URL);
    gloo_net::http::Request::post(&url)
        .header("Content-Type", "application/json")
        .body(
            serde_json::to_string(payload)
                .map_err(|e| format!("Failed to serialize payload: {}", e))?,
        )
        .map_err(|e| format!("Failed to build request: {}", e))?
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

pub async fn fetch_llm_config() -> Result<LlmConfigResponse, String> {
    fetch_json::<LlmConfigResponse>("/config/llm").await
}

pub async fn commit_llm_config(payload: &LlmConfig) -> Result<LlmConfigResponse, String> {
    let url = format!("{}/config/llm", API_BASE_URL);
    gloo_net::http::Request::post(&url)
        .header("Content-Type", "application/json")
        .body(
            serde_json::to_string(payload)
                .map_err(|e| format!("Failed to serialize payload: {}", e))?,
        )
        .map_err(|e| format!("Failed to build request: {}", e))?
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

pub async fn fetch_hardware_config() -> Result<HardwareConfigResponse, String> {
    fetch_json::<HardwareConfigResponse>("/config/hardware").await
}

pub async fn commit_hardware_config(
    payload: &HardwareConfig,
) -> Result<HardwareConfigResponse, String> {
    let url = format!("{}/config/hardware", API_BASE_URL);
    gloo_net::http::Request::post(&url)
        .header("Content-Type", "application/json")
        .body(
            serde_json::to_string(payload)
                .map_err(|e| format!("Failed to serialize payload: {}", e))?,
        )
        .map_err(|e| format!("Failed to build request: {}", e))?
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

pub async fn fetch_api_keys() -> Result<ApiKeysResponse, String> {
    fetch_json::<ApiKeysResponse>("/config/api_keys").await
}

pub async fn save_api_keys(payload: &ApiKeysRequest) -> Result<ApiKeysResponse, String> {
    let url = format!("{}/config/api_keys", API_BASE_URL);
    gloo_net::http::Request::post(&url)
        .header("Content-Type", "application/json")
        .body(
            serde_json::to_string(payload)
                .map_err(|e| format!("Failed to serialize payload: {}", e))?,
        )
        .map_err(|e| format!("Failed to build request: {}", e))?
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

pub async fn delete_api_key(provider: &str) -> Result<serde_json::Value, String> {
    let url = format!("{}/config/api_keys/{}", API_BASE_URL, provider);
    gloo_net::http::Request::delete(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

pub async fn fetch_reindex_status(job_id: &str) -> Result<ReindexStatusResponse, String> {
    let url = format!("{}/reindex/status/{}", API_BASE_URL, job_id);

    gloo_net::http::Request::get(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

/// Fetch request metrics snapshot for the Monitor UI
pub async fn fetch_requests_snapshot() -> Result<RequestsSnapshot, String> {
    let url = format!("{}/monitoring/ui/requests", API_BASE_URL);

    gloo_net::http::Request::get(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

/// Fetch index info for the monitor page
pub async fn fetch_index_info() -> Result<IndexInfoResponse, String> {
    fetch_json::<IndexInfoResponse>("/index/info").await
}

pub async fn get_chunking_logging() -> Result<ChunkingLoggingResponse, String> {
    fetch_json::<ChunkingLoggingResponse>("/monitoring/chunking/logging").await
}

pub async fn set_chunking_logging(enabled: bool) -> Result<ChunkingLoggingResponse, String> {
    let url = format!("/monitoring/chunking/logging?enabled={}", enabled);
    fetch_json::<ChunkingLoggingResponse>(&url).await
}

pub async fn fetch_cache_info() -> Result<CacheInfoResponse, String> {
    let url = format!("{}/monitor/cache/info", API_BASE_URL);
    let response = gloo_net::http::Request::get(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?;

    if response.status() == 204 {
        return Err("Backend returned 204 No Content for cache info".into());
    }

    response
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

pub async fn fetch_rate_limit_info() -> Result<RateLimitInfoResponse, String> {
    let url = format!("{}/monitor/rate_limits/info", API_BASE_URL);
    gloo_net::http::Request::get(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct SetRateLimitEnabledResponse {
    pub request_id: String,
    pub enabled: bool,
    pub message: String,
}

pub async fn set_rate_limit_enabled(enabled: bool) -> Result<SetRateLimitEnabledResponse, String> {
    let url = format!("{}/monitor/rate_limits/enabled", API_BASE_URL);
    gloo_net::http::Request::post(&url)
        .header("Content-Type", "application/json")
        .body(serde_json::json!({ "enabled": enabled }).to_string())
        .map_err(|e| format!("Failed to create request: {:?}", e))?
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

pub async fn fetch_recent_logs(limit: usize) -> Result<LogsResponse, String> {
    let url = format!("{}/monitor/logs/recent?limit={}", API_BASE_URL, limit);
    gloo_net::http::Request::get(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct UploadResponse {
    pub status: String,
    #[serde(default)]
    pub uploaded_files: Vec<String>,
    #[serde(default)]
    pub indexed_files: Vec<String>,
    #[serde(default)]
    pub index_errors: Vec<String>,
}

pub async fn upload_document(filename: &str, data: &[u8]) -> Result<UploadResponse, String> {
    use gloo_net::http::Request;
    use js_sys::{Array, Uint8Array};
    use web_sys::{Blob, BlobPropertyBag, FormData};

    let url = format!("{}/upload", API_BASE_URL);

    // Create a Uint8Array from the data
    let uint8_array = Uint8Array::new_with_length(data.len() as u32);
    uint8_array.copy_from(data);

    // Create blob from the array
    let array = Array::new();
    array.push(&uint8_array);
    let mut blob_options = BlobPropertyBag::new();
    blob_options.type_("application/octet-stream");
    let blob = Blob::new_with_u8_array_sequence_and_options(&array, &blob_options)
        .map_err(|_| "Failed to create blob".to_string())?;

    // Create FormData and append the blob with filename
    let form_data = FormData::new().map_err(|_| "Failed to create FormData".to_string())?;
    form_data
        .append_with_blob_and_filename("file", &blob, filename)
        .map_err(|_| "Failed to append file to FormData".to_string())?;

    // Send the request
    let response = Request::post(&url)
        .body(form_data)
        .map_err(|e| format!("Failed to create request: {:?}", e))?
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?;

    let status = response.status();
    if !(200..=299).contains(&status) {
        let body = response.text().await.unwrap_or_default();
        return Err(format!("HTTP {}: {}", status, body));
    }

    response
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))
}

async fn fetch_json<T>(path: &str) -> Result<T, String>
where
    T: for<'de> serde::Deserialize<'de>,
{
    let url = format!("{}{}", API_BASE_URL, path);
    let response = gloo_net::http::Request::get(&url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?;

    let status = response.status();
    if !(200..=299).contains(&status) {
        let body = match response.text().await {
            Ok(body) => body.trim().to_string(),
            Err(_) => String::new(),
        };
        let detail = if body.is_empty() {
            "(empty response)".to_string()
        } else {
            body
        };
        return Err(format!("HTTP {} {}", status, detail));
    }

    response
        .json::<T>()
        .await
        .map_err(|e| format!("Failed to parse JSON: {}", e))
}
