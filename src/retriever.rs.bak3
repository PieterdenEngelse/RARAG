use tantivy::{
    Index, 
    collector::TopDocs, 
    query::QueryParser,
    schema::{Schema, TEXT, STORED, Field, Value},
    directory::MmapDirectory,
    TantivyError,
    query::QueryParserError,
    directory::error::OpenDirectoryError,
    IndexWriter,
};
use serde::{Serialize, Deserialize};
use std::fmt;
use std::collections::HashMap;
use std::fs::File;
use std::io::{Write, Read};
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, Ordering};
use rayon::prelude::*;
use lru::LruCache;
use std::num::NonZeroUsize;
use std::time::{Instant, Duration};
use std::path::Path;
use tracing::{info, error};
use std::fs; 
use fs2;

/// Custom error type for Retriever operations
#[derive(Debug, Serialize, Deserialize)]
pub enum RetrieverError {
    TantivyError(String),
    IoError(String),
    IndexError(String),
    VectorError(String),
    QueryParserError(String),
    DirectoryError(String),
    SerializationError(String),
}

impl fmt::Display for RetrieverError {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self {
            RetrieverError::TantivyError(e) => write!(f, "Tantivy error: {}", e),
            RetrieverError::IoError(e) => write!(f, "IO error: {}", e),
            RetrieverError::IndexError(msg) => write!(f, "Index error: {}", msg),
            RetrieverError::VectorError(msg) => write!(f, "Vector error: {}", msg),
            RetrieverError::QueryParserError(msg) => write!(f, "Query parser error: {}", msg),
            RetrieverError::DirectoryError(msg) => write!(f, "Directory error: {}", msg),
            RetrieverError::SerializationError(msg) => write!(f, "Serialization error: {}", msg),
        }
    }
}

impl std::error::Error for RetrieverError {}

impl From<TantivyError> for RetrieverError {
    fn from(err: TantivyError) -> Self {
        RetrieverError::TantivyError(err.to_string())
    }
}

impl From<std::io::Error> for RetrieverError {
    fn from(err: std::io::Error) -> Self {
        RetrieverError::IoError(err.to_string())
    }
}

impl From<QueryParserError> for RetrieverError {
    fn from(err: QueryParserError) -> Self {
        RetrieverError::QueryParserError(err.to_string())
    }
}

impl From<OpenDirectoryError> for RetrieverError {
    fn from(err: OpenDirectoryError) -> Self {
        RetrieverError::DirectoryError(err.to_string())
    }
}

#[derive(Serialize, Deserialize)]
struct VectorStorage {
    vectors: Vec<Vec<f32>>,
    doc_id_to_vector_idx: HashMap<String, usize>,
}

/// Metrics for monitoring Retriever performance
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RetrieverMetrics {
    /// Total number of search operations performed
    pub total_searches: usize,
    /// Total number of cache hits
    pub cache_hits: usize,
    /// Total number of cache misses
    pub cache_misses: usize,
    /// Average search latency in microseconds
    pub avg_search_latency_us: f64,
    /// Total search latency accumulated (for calculating average)
    pub total_search_latency_us: u128,
    /// Maximum search latency observed
    pub max_search_latency_us: u128,
    /// Total number of documents indexed
    pub total_documents_indexed: usize,
    /// Total number of vectors stored
    pub total_vectors: usize,
    /// Index directory path
    pub index_path: String,
    /// Last update timestamp
    pub last_updated: u64,
}

impl Default for RetrieverMetrics {
    fn default() -> Self {
        Self {
            total_searches: 0,
            cache_hits: 0,
            cache_misses: 0,
            avg_search_latency_us: 0.0,
            total_search_latency_us: 0,
            max_search_latency_us: 0,
            total_documents_indexed: 0,
            total_vectors: 0,
            index_path: String::new(),
            last_updated: 0,
        }
    }
}

impl RetrieverMetrics {
    /// Calculate cache hit rate (0.0 to 1.0)
    pub fn cache_hit_rate(&self) -> f64 {
        if self.total_searches == 0 {
            0.0
        } else {
            self.cache_hits as f64 / self.total_searches as f64
        }
    }

    /// Get index size in bytes
    pub fn get_index_size_bytes(&self) -> Result<u64, std::io::Error> {
        let path = Path::new(&self.index_path);
        if !path.exists() {
            return Ok(0);
        }
        let mut total_size = 0;
        for entry in std::fs::read_dir(path)? {
            let entry = entry?;
            let metadata = entry.metadata()?;
            if metadata.is_file() {
                total_size += metadata.len();
            }
        }
        Ok(total_size)
    }

    /// Get human-readable index size
    pub fn get_index_size_human(&self) -> Result<String, std::io::Error> {
        let size = self.get_index_size_bytes()?;
        let sizes = ["B", "KB", "MB", "GB", "TB"];
        let mut size = size as f64;
        let mut i = 0;
        while size >= 1024.0 && i < sizes.len() - 1 {
            size /= 1024.0;
            i += 1;
        }
        Ok(format!("{:.2} {}", size, sizes[i]))
    }
}

/// Retriever stores vectors and handles search operations.
pub struct Retriever {
    pub vectors: Vec<Vec<f32>>,
    pub index: Index,
    pub title_field: Field,
    pub content_field: Field,
    pub doc_id_to_vector_idx: HashMap<String, usize>,
    pub vector_file_path: String,
    pub auto_save_threshold: usize,
    documents_since_save: Arc<AtomicUsize>,
    index_writer: Option<IndexWriter>,
    batch_mode: bool,
    search_cache: LruCache<String, Vec<String>>,
    cache_enabled: bool,
    /// Metrics for monitoring performance
    pub metrics: RetrieverMetrics,
    /// Index directory path for size calculations
    index_dir_path: String,
}

// Helper function for cosine similarity
fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let magnitude_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let magnitude_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    if magnitude_a == 0.0 || magnitude_b == 0.0 {
        0.0
    } else {
        dot_product / (magnitude_a * magnitude_b)
    }
}

impl Retriever {
    pub fn new(index_dir: &str) -> Result<Self, RetrieverError> {
        let mut schema_builder = Schema::builder();
        let title_field = schema_builder.add_text_field("title", TEXT | STORED);
        let content_field = schema_builder.add_text_field("content", TEXT | STORED);
        let schema = schema_builder.build();
        // Ensure the index directory exists
        fs::create_dir_all(index_dir)?;
        let dir = MmapDirectory::open(index_dir)?;
        let index = Index::open_or_create(dir, schema)?;
        let mut retriever = Retriever {
            vectors: Vec::new(),
            index,
            title_field,
            content_field,
            doc_id_to_vector_idx: HashMap::new(),
            vector_file_path: "./vectors.json".to_string(),
            auto_save_threshold: 100,
            documents_since_save: Arc::new(AtomicUsize::new(0)),
            index_writer: None,
            batch_mode: false,
            search_cache: LruCache::new(NonZeroUsize::new(100).unwrap()),
            cache_enabled: true,
            metrics: RetrieverMetrics {
                index_path: index_dir.to_string(),
                ..Default::default()
            },
            index_dir_path: index_dir.to_string(),
        };
        // Try to load existing vectors on startup
        if let Err(e) = retriever.load_vectors(&retriever.vector_file_path.clone()) {
            info!("No existing vectors found, starting fresh: {}", e);
        } else {
            info!("Loaded existing vectors from {}", retriever.vector_file_path);
            retriever.metrics.total_vectors = retriever.vectors.len();
        }
        // Update metrics with current document count
        if let Ok(reader) = retriever.index.reader() {
            retriever.metrics.total_documents_indexed = reader.searcher().num_docs() as usize;
        }
        Ok(retriever)
    }

    pub fn new_dummy() -> Result<Self, RetrieverError> {
        Self::new("./tantivy_index")
    }

    /// Enable or disable search caching
    pub fn set_cache_enabled(&mut self, enabled: bool) {
        self.cache_enabled = enabled;
        if !enabled {
            self.search_cache.clear();
        }
        info!("Search cache {}", if enabled { "enabled" } else { "disabled" });
    }

    /// Clear the search cache
    pub fn clear_cache(&mut self) {
        self.search_cache.clear();
        info!("Search cache cleared");
    }

    /// Get cache statistics
    pub fn cache_stats(&self) -> (usize, usize) {
        (self.search_cache.len(), self.search_cache.cap().get())
    }

    /// Get current metrics
    pub fn get_metrics(&self) -> RetrieverMetrics {
        self.metrics.clone()
    }

    /// Reset all metrics (useful for testing or periodic resets)
    pub fn reset_metrics(&mut self) {
        self.metrics = RetrieverMetrics {
            index_path: self.index_dir_path.clone(),
            ..Default::default()
        };
    }

    /// Start batch indexing mode - creates a single writer for multiple documents
    pub fn begin_batch(&mut self) -> Result<(), RetrieverError> {
        if self.index_writer.is_some() {
            return Err(RetrieverError::IndexError("Batch already in progress".to_string()));
        }
        self.index_writer = Some(self.index.writer(50_000_000)?);
        self.batch_mode = true;
        info!("Batch indexing mode started");
        Ok(())
    }

    /// End batch indexing mode - commits all pending documents
    pub fn end_batch(&mut self) -> Result<(), RetrieverError> {
        if let Some(mut writer) = self.index_writer.take() {
            writer.commit()?;
            self.batch_mode = false;
            // Clear cache after indexing
            self.clear_cache();
            // Update metrics
            if let Ok(reader) = self.index.reader() {
                self.metrics.total_documents_indexed = reader.searcher().num_docs() as usize;
            }
            info!("Batch indexing mode ended, changes committed");
            Ok(())
        } else {
            Err(RetrieverError::IndexError("No batch in progress".to_string()))
        }
    }

    /// Add multiple documents in batch mode
    pub fn add_documents_batch(&mut self, documents: Vec<(String, String)>) -> Result<usize, RetrieverError> {
        let was_batch = self.batch_mode;
        if !was_batch {
            self.begin_batch()?;
        }
        let mut count = 0;
        for (title, content) in documents {
            if let Err(e) = self.add_document_to_batch(&title, &content) {
                error!("Failed to add document '{}': {}", title, e);
            } else {
                count += 1;
            }
        }
        if !was_batch {
            self.end_batch()?;
        }
        // Update metrics
        self.metrics.total_documents_indexed += count;
        Ok(count)
    }

    /// Add a document to the current batch (internal use)
    fn add_document_to_batch(&mut self, title: &str, content: &str) -> Result<(), RetrieverError> {
        if !self.batch_mode {
            return Err(RetrieverError::IndexError("Not in batch mode".to_string()));
        }
        let mut doc = tantivy::TantivyDocument::default();
        doc.add_text(self.title_field, title);
        doc.add_text(self.content_field, content);
        if let Some(writer) = &mut self.index_writer {
            writer.add_document(doc)?;
            Ok(())
        } else {
            Err(RetrieverError::IndexError("No writer available".to_string()))
        }
    }

    pub fn search(&mut self, query_str: &str) -> Result<Vec<String>, RetrieverError> {
        let start_time = Instant::now();
        // Check cache first
        if self.cache_enabled {
            if let Some(cached) = self.search_cache.get(query_str) {
                self.metrics.cache_hits += 1;
                self.metrics.total_searches += 1;
                let latency_us = start_time.elapsed().as_micros();
                self.metrics.total_search_latency_us += latency_us;
                self.metrics.avg_search_latency_us = 
                    self.metrics.total_search_latency_us as f64 / self.metrics.total_searches as f64;
                if latency_us > self.metrics.max_search_latency_us {
                    self.metrics.max_search_latency_us = latency_us;
                }
                self.metrics.last_updated = std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or(Duration::from_secs(0))
                    .as_secs();
                return Ok(cached.clone());
            }
        }
        // Cache miss
        self.metrics.cache_misses += 1;
        self.metrics.total_searches += 1;
        let reader = self.index.reader()?;
        let searcher = reader.searcher();
        let parser = QueryParser::for_index(&self.index, vec![self.title_field, self.content_field]);
        let query = parser.parse_query(query_str)?;
        let top_docs = searcher.search(&query, &TopDocs::with_limit(10))?;
        let mut results = Vec::new();
        for (_score, doc_address) in top_docs {
            let doc = searcher.doc::<tantivy::TantivyDocument>(doc_address)?;
            let content = doc
                .get_first(self.content_field)
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .to_string();
            results.push(content);
        }
        // Cache the results
        if self.cache_enabled {
            self.search_cache.put(query_str.to_string(), results.clone());
        }
        // Update metrics
        let latency_us = start_time.elapsed().as_micros();
        self.metrics.total_search_latency_us += latency_us;
        self.metrics.avg_search_latency_us = 
            self.metrics.total_search_latency_us as f64 / self.metrics.total_searches as f64;
        if latency_us > self.metrics.max_search_latency_us {
            self.metrics.max_search_latency_us = latency_us;
        }
        self.metrics.last_updated = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or(Duration::from_secs(0))
            .as_secs();
        Ok(results)
    }

    pub fn add_vector(&mut self, vector: Vec<f32>) {
        self.vectors.push(vector);
        self.metrics.total_vectors += 1;
        self.check_auto_save();
    }

    pub fn add_vector_with_id(&mut self, doc_id: String, vector: Vec<f32>) {
        let idx = self.vectors.len();
        self.vectors.push(vector);
        self.doc_id_to_vector_idx.insert(doc_id, idx);
        self.metrics.total_vectors += 1;
        self.check_auto_save();
    }

    fn check_auto_save(&mut self) {
        let count = self.documents_since_save.fetch_add(1, Ordering::SeqCst) + 1;
        if count >= self.auto_save_threshold {
            if let Err(e) = self.save_vectors(&self.vector_file_path.clone()) {
                error!("Auto-save failed: {}", e);
            } else {
                info!("Auto-saved vectors after {} documents", count);
                self.documents_since_save.store(0, Ordering::SeqCst);
            }
        }
    }

    /// Parallel vector search using Rayon
    pub fn vector_search(&self, query_vector: &[f32], top_k: usize) -> Vec<(usize, f32)> {
        let use_parallel = self.vectors.len() > 1000;
        let mut similarities: Vec<(usize, f32)> = if use_parallel {
            self.vectors
                .par_iter()
                .enumerate()
                .map(|(idx, vec)| (idx, cosine_similarity(query_vector, vec)))
                .collect()
        } else {
            self.vectors
                .iter()
                .enumerate()
                .map(|(idx, vec)| (idx, cosine_similarity(query_vector, vec)))
                .collect()
        };
        similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        similarities.into_iter().take(top_k).collect()
    }

    pub fn hybrid_search(&mut self, query: &str, query_vector: Option<&[f32]>) -> Result<Vec<String>, RetrieverError> {
        // Get keyword search results (metrics updated inside search())
        let keyword_results = self.search(query)?;
        // If no query vector provided, return keyword results only
        let query_vec = match query_vector {
            Some(v) => v,
            None => return Ok(keyword_results),
        };
        // Get vector search results
        let vector_results = self.vector_search(query_vec, 10);
        // Merge results using Reciprocal Rank Fusion (RRF)
        let k = 60.0; // RRF constant
        let mut score_map: HashMap<String, f32> = HashMap::new();
        // Score keyword results
        for (rank, content) in keyword_results.iter().enumerate() {
            let score = 1.0 / (k + (rank as f32) + 1.0);
            *score_map.entry(content.clone()).or_insert(0.0) += score;
        }
        // Score vector results (map indices back to content)
        for (rank, (idx, _similarity)) in vector_results.iter().enumerate() {
            // Try to find the document content for this vector index
            if let Some(content) = self.get_content_by_vector_idx(*idx) {
                let score = 1.0 / (k + (rank as f32) + 1.0);
                *score_map.entry(content).or_insert(0.0) += score;
            }
        }
        // Sort by combined score
        let mut merged_results: Vec<(String, f32)> = score_map.into_iter().collect();
        merged_results.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        // Return top 10 results
        Ok(merged_results.into_iter().take(10).map(|(content, _)| content).collect())
    }

    fn get_content_by_vector_idx(&self, idx: usize) -> Option<String> {
        // Find document ID for this vector index
        for (doc_id, &vec_idx) in &self.doc_id_to_vector_idx {
            if vec_idx == idx {
                return Some(doc_id.clone());
            }
        }
        None
    }

    // Original API-compatible version (for backward compatibility)
    pub fn rerank_by_similarity(&self, _query: &str, candidates: &Vec<String>) -> Vec<String> {
        let mut results = candidates.clone();
        results.reverse(); // simple placeholder
        results
    }

    // New vector-based reranking
    pub fn rerank_by_vector_similarity(&self, query_vector: &[f32], candidate_indices: &[usize]) -> Result<Vec<(usize, f32)>, RetrieverError> {
        let mut scored_candidates: Vec<(usize, f32)> = candidate_indices
            .iter()
            .filter_map(|&idx| {
                if idx < self.vectors.len() {
                    Some((idx, cosine_similarity(query_vector, &self.vectors[idx])))
                } else {
                    None
                }
            })
            .collect();
        if scored_candidates.is_empty() && !candidate_indices.is_empty() {
            return Err(RetrieverError::VectorError("No valid candidate indices found".to_string()));
        }
        // Sort by similarity score (descending)
        scored_candidates.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        Ok(scored_candidates)
    }

    pub fn summarize_chunks(&self, _query: &str, candidates: &Vec<String>) -> String {
        format!("Summary for {} chunks", candidates.len())
    }

    pub fn index_document(&mut self, doc: impl tantivy::Document) -> Result<(), RetrieverError> {
        let mut index_writer = self.index.writer(50_000_000)?;
        index_writer.add_document(doc)?;
        index_writer.commit()?;
        // Clear cache after indexing
        self.clear_cache();
        // Update metrics
        self.metrics.total_documents_indexed += 1;
        Ok(())
    }

    pub fn add_document(&mut self, title: &str, content: &str) -> Result<(), RetrieverError> {
        // If in batch mode, use batch method
        if self.batch_mode {
            return self.add_document_to_batch(title, content);
        }
        // Otherwise, create single-use writer
        let mut doc = tantivy::TantivyDocument::default();
        doc.add_text(self.title_field, title);
        doc.add_text(self.content_field, content);
        let mut index_writer = self.index.writer(50_000_000)?;
        index_writer.add_document(doc)?;
        index_writer.commit()?;
        // Clear cache after indexing
        self.clear_cache();
        // Update metrics
        self.metrics.total_documents_indexed += 1;
        Ok(())
    }

    pub fn commit(&mut self) -> Result<(), RetrieverError> {
        // If in batch mode, end it
        if self.batch_mode {
            self.end_batch()?;
        }
        // Force save vectors
        self.save_vectors(&self.vector_file_path.clone())?;
        Ok(())
    }

    pub fn save_vectors(&self, filename: &str) -> Result<(), RetrieverError> {
        let storage = VectorStorage {
            vectors: self.vectors.clone(),
            doc_id_to_vector_idx: self.doc_id_to_vector_idx.clone(),
        };
        let json = serde_json::to_string(&storage)
            .map_err(|e| RetrieverError::SerializationError(e.to_string()))?;
        let mut file = File::create(filename)?;
        file.write_all(json.as_bytes())?;
        Ok(())
    }

    pub fn load_vectors(&mut self, filename: &str) -> Result<(), RetrieverError> {
        let mut file = File::open(filename)?;
        let mut contents = String::new();
        file.read_to_string(&mut contents)?;
        let storage: VectorStorage = serde_json::from_str(&contents)
            .map_err(|e| RetrieverError::SerializationError(e.to_string()))?;
        self.vectors = storage.vectors;
        self.doc_id_to_vector_idx = storage.doc_id_to_vector_idx;
        self.metrics.total_vectors = self.vectors.len();
        Ok(())
    }

    pub fn force_save(&self) -> Result<(), RetrieverError> {
        info!("Manual save triggered");
        self.save_vectors(&self.vector_file_path)
    }

    pub fn set_auto_save_threshold(&mut self, threshold: usize) {
        self.auto_save_threshold = threshold;
        info!("Auto-save threshold set to {} documents", threshold);
    }

    pub fn index_chunk(&mut self, chunk_id: &str, chunk_text: &str, vector: &Vec<f32>) -> Result<(), RetrieverError> {
        // Add the document to Tantivy index
        self.add_document(chunk_id, chunk_text)?;
        // Add vector with document ID mapping
        self.add_vector_with_id(chunk_text.to_string(), vector.clone());
        Ok(())
    }

    /// Check if there's sufficient disk space available
    fn check_disk_space(&self, min_free_bytes: u64) -> Result<(), RetrieverError> {
        let path = Path::new(&self.index_dir_path);
        let available_space = fs2::available_space(path)
            .map_err(|e| RetrieverError::IoError(format!("Failed to get disk space: {}", e)))?;
        if available_space < min_free_bytes {
            return Err(RetrieverError::IoError(
                format!("Insufficient disk space: {} bytes available, {} bytes required", 
                       available_space, min_free_bytes)
            ));
        }
        Ok(())
    }

    /// Performs a comprehensive health check of the Retriever system
    /// 
    /// This method verifies:
    /// - Index directory accessibility and basic integrity
    /// - Vector storage consistency (vectors and mapping alignment)
    /// - Basic search functionality with a simple query
    /// - Cache functionality (if enabled)
    /// 
    /// Returns Ok(()) if all checks pass, or a RetrieverError with details if any check fails.
    pub fn health_check(&self) -> Result<(), RetrieverError> {
        // Check 1: Verify index directory is accessible
        let index_path = Path::new(&self.index_dir_path);
        if !index_path.exists() {
            return Err(RetrieverError::DirectoryError(
                format!("Index directory does not exist: {}", self.index_dir_path)
            ));
        }
        if !index_path.is_dir() {
            return Err(RetrieverError::DirectoryError(
                format!("Index path is not a directory: {}", self.index_dir_path)
            ));
        }

        // Check 2: Verify index reader can be created
        let reader = self.index.reader()
            .map_err(|e| RetrieverError::IndexError(format!("Failed to create index reader: {}", e)))?;
        let searcher = reader.searcher();
        let doc_count = searcher.num_docs();

        // Check 3: Verify vector storage consistency
        if self.vectors.len() != self.doc_id_to_vector_idx.len() {
            return Err(RetrieverError::VectorError(
                format!("Vector storage inconsistency: {} vectors but {} document mappings", 
                       self.vectors.len(), self.doc_id_to_vector_idx.len())
            ));
        }

        // Check 4: Verify all vector indices in the mapping are valid
        for (doc_id, &vec_idx) in &self.doc_id_to_vector_idx {
            if vec_idx >= self.vectors.len() {
                return Err(RetrieverError::VectorError(
                    format!("Invalid vector index {} for document '{}' (vectors length: {})", 
                           vec_idx, doc_id, self.vectors.len())
                ));
            }
        }

        // Check 4B: Validate vector dimensions
        self.validate_vector_dimensions()?;

        // Check 5: Test basic search functionality if there are documents
        if doc_count > 0 {
            let parser = QueryParser::for_index(&self.index, vec![self.title_field, self.content_field]);
            match parser.parse_query("*") {
                Ok(query) => {
                    if let Err(e) = searcher.search(&query, &TopDocs::with_limit(1)) {
                        return Err(RetrieverError::IndexError(
                            format!("Basic search test failed: {}", e)
                        ));
                    }
                }
                Err(_e) => {
                    let fallback_query = parser.parse_query("a").unwrap_or_else(|_| parser.parse_query("*").unwrap());
                    let first_doc_addr = searcher
                        .search(&fallback_query, &TopDocs::with_limit(1))
                        .map(|top_docs| top_docs.first().map(|(_, addr)| *addr))
                        .unwrap_or(None);
                    if let Some(addr) = first_doc_addr {
                        if let Err(e) = searcher.doc::<tantivy::TantivyDocument>(addr) {
                            return Err(RetrieverError::IndexError(
                                format!("Failed to retrieve document: {}", e)
                            ));
                        }
                    }
                }
            }
        }

        // Check 6: Verify cache integrity (if enabled and has entries)
        if self.cache_enabled && !self.search_cache.is_empty() {
            let _ = self.search_cache.len();
            let _ = self.search_cache.cap();
        }

        // Check 7: Verify vector file path is writable
        if Path::new(&self.vector_file_path).exists() {
            if let Err(e) = std::fs::OpenOptions::new().write(true).open(&self.vector_file_path) {
                return Err(RetrieverError::IoError(
                    format!("Vector file exists but is not writable: {}", e)
                ));
            }
        } else {
            if let Some(parent) = Path::new(&self.vector_file_path).parent() {
                if !parent.exists() {
                    return Err(RetrieverError::IoError(
                        format!("Vector file parent directory does not exist: {:?}", parent)
                    ));
                }
                let temp_file = parent.join(".health_check_test");
                if let Err(e) = std::fs::File::create(&temp_file) {
                    let _ = std::fs::remove_file(&temp_file);
                    return Err(RetrieverError::IoError(
                        format!("Cannot write to vector file directory: {}", e)
                    ));
                }
                let _ = std::fs::remove_file(&temp_file);
            }
        }

        // Check 8: Verify sufficient disk space (e.g., 100MB minimum)
        self.check_disk_space(100 * 1024 * 1024)?;

        info!("Health check passed - {} documents, {} vectors", doc_count, self.vectors.len());
        Ok(())
    }

    /// Validates that all vectors have the expected dimension
    fn validate_vector_dimensions(&self) -> Result<(), RetrieverError> {
        if self.vectors.is_empty() {
            return Ok(()); // No vectors to validate
        }
        let expected_dim = self.vectors[0].len();
        for (idx, vector) in self.vectors.iter().enumerate() {
            if vector.len() != expected_dim {
                return Err(RetrieverError::VectorError(
                    format!("Vector dimension mismatch at index {}: expected {}, found {}", 
                           idx, expected_dim, vector.len())
                ));
            }
        }
        info!("Vector dimension validation passed: {} vectors with dimension {}", 
              self.vectors.len(), expected_dim);
        Ok(())
    }

    /// Performs a lightweight readiness check
    /// 
    /// This method only verifies that the service can handle requests:
    /// - Index reader can be created
    /// - Basic vector storage is accessible
    /// 
    /// This is faster than health_check and suitable for load balancer checks.
    pub fn ready_check(&self) -> Result<(), RetrieverError> {
        let reader = self.index.reader()
            .map_err(|e| RetrieverError::IndexError(format!("Failed to create index reader: {}", e)))?;
        let _searcher = reader.searcher();
        let _ = self.vectors.len();
        let _ = self.doc_id_to_vector_idx.len();
        Ok(())
    }
}

// Implement Drop to save on shutdown
impl Drop for Retriever {
    fn drop(&mut self) {
        // End batch mode if active
        if self.batch_mode {
            if let Err(e) = self.end_batch() {
                error!("Failed to end batch on shutdown: {}", e);
            }
        }
        info!("Retriever shutting down, saving vectors...");
        if let Err(e) = self.save_vectors(&self.vector_file_path.clone()) {
            error!("Failed to save vectors on shutdown: {}", e);
        } else {
            info!("Vectors saved successfully on shutdown");
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use std::sync::atomic::{AtomicUsize, Ordering};
    static TEST_COUNTER: AtomicUsize = AtomicUsize::new(0);

    fn setup_test_retriever() -> (Retriever, String) {
        let test_id = TEST_COUNTER.fetch_add(1, Ordering::SeqCst);
        let test_dir = format!("./test_tantivy_index_{}", test_id);
        let test_vector_file = format!("./test_vectors_{}.json", test_id);
        let _ = fs::remove_dir_all(&test_dir);
        fs::create_dir_all(&test_dir).unwrap();
        let mut retriever = Retriever::new(&test_dir).unwrap();
        retriever.vector_file_path = test_vector_file;
        retriever.auto_save_threshold = 1000; // Disable auto-save for tests
        (retriever, test_dir)
    }

    fn cleanup_test_files(test_dir: &str, vector_file: &str) {
        let _ = fs::remove_file(vector_file);
        let _ = fs::remove_dir_all(test_dir);
    }

    #[test]
    fn test_cosine_similarity() {
        let v1 = vec![1.0, 0.0, 0.0];
        let v2 = vec![1.0, 0.0, 0.0];
        let v3 = vec![0.0, 1.0, 0.0];
        assert!((cosine_similarity(&v1, &v2) - 1.0).abs() < 0.001);
        assert!((cosine_similarity(&v1, &v3)).abs() < 0.001);
    }

    #[test]
    fn test_add_and_search_document() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.add_document("Test Title", "This is test content about Rust").unwrap();
        let results = retriever.search("Rust").unwrap();
        assert!(!results.is_empty());
        assert_eq!(results[0], "This is test content about Rust");
        let metrics = retriever.get_metrics();
        assert_eq!(metrics.total_documents_indexed, 1);
        assert_eq!(metrics.total_searches, 1);
        assert_eq!(metrics.cache_misses, 1);
        assert!(metrics.avg_search_latency_us > 0.0);
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_batch_indexing() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        let docs = vec![
            ("Doc1".to_string(), "First document".to_string()),
            ("Doc2".to_string(), "Second document".to_string()),
            ("Doc3".to_string(), "Third document".to_string()),
        ];
        let count = retriever.add_documents_batch(docs).unwrap();
        assert_eq!(count, 3);
        let results = retriever.search("document").unwrap();
        assert_eq!(results.len(), 3);
        let metrics = retriever.get_metrics();
        assert_eq!(metrics.total_documents_indexed, 3);
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_vector_operations() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.add_vector(vec![1.0, 0.0, 0.0]);
        retriever.add_vector(vec![0.9, 0.1, 0.0]);
        retriever.add_vector(vec![0.0, 1.0, 0.0]);
        let query = vec![1.0, 0.0, 0.0];
        let results = retriever.vector_search(&query, 2);
        assert_eq!(results.len(), 2);
        assert_eq!(results[0].0, 0); // First vector is most similar
        assert!(results[0].1 > results[1].1); // Check ordering
        let metrics = retriever.get_metrics();
        assert_eq!(metrics.total_vectors, 3);
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_save_and_load_vectors() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.add_vector_with_id("doc1".to_string(), vec![1.0, 2.0, 3.0]);
        retriever.add_vector_with_id("doc2".to_string(), vec![4.0, 5.0, 6.0]);
        retriever.save_vectors(&vector_file).unwrap();
        let test_id = TEST_COUNTER.fetch_add(1, Ordering::SeqCst);
        let test_dir2 = format!("./test_tantivy_index_{}", test_id);
        let _ = fs::create_dir_all(&test_dir2);
        let mut retriever2 = Retriever::new(&test_dir2).unwrap();
        retriever2.vector_file_path = vector_file.clone();
        retriever2.load_vectors(&vector_file).unwrap();
        assert_eq!(retriever2.vectors.len(), 2);
        assert_eq!(retriever2.doc_id_to_vector_idx.len(), 2);
        assert_eq!(retriever2.metrics.total_vectors, 2);
        drop(retriever);
        drop(retriever2);
        cleanup_test_files(&test_dir, &vector_file);
        cleanup_test_files(&test_dir2, &vector_file);
    }

    #[test]
    fn test_rerank_by_vector_similarity() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.add_vector(vec![1.0, 0.0, 0.0]);
        retriever.add_vector(vec![0.0, 1.0, 0.0]);
        retriever.add_vector(vec![0.9, 0.1, 0.0]);
        let query = vec![1.0, 0.0, 0.0];
        let candidates = vec![0, 1, 2];
        let results = retriever.rerank_by_vector_similarity(&query, &candidates).unwrap();
        assert_eq!(results.len(), 3);
        assert_eq!(results[0].0, 0); // Most similar
        assert_eq!(results[1].0, 2); // Second most similar
        assert_eq!(results[2].0, 1); // Least similar
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_batch_mode_errors() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        let result = retriever.add_document_to_batch("title", "content");
        assert!(result.is_err());
        retriever.begin_batch().unwrap();
        let result = retriever.begin_batch();
        assert!(result.is_err());
        retriever.add_document_to_batch("title", "content").unwrap();
        retriever.end_batch().unwrap();
        let result = retriever.end_batch();
        assert!(result.is_err());
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_hybrid_search_without_vectors() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.add_document("Test", "Rust programming").unwrap();
        let results = retriever.hybrid_search("Rust", None).unwrap();
        assert!(!results.is_empty());
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_index_chunk() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        let vector = vec![1.0, 2.0, 3.0];
        retriever.index_chunk("chunk1", "This is a chunk", &vector).unwrap();
        assert_eq!(retriever.vectors.len(), 1);
        let results = retriever.search("chunk").unwrap();
        assert!(!results.is_empty());
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_search_caching() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.add_document("Test", "Rust programming").unwrap();
        let results1 = retriever.search("Rust").unwrap();
        let results2 = retriever.search("Rust").unwrap();
        assert_eq!(results1, results2);
        let (cache_size, _) = retriever.cache_stats();
        assert_eq!(cache_size, 1);
        let metrics = retriever.get_metrics();
        assert_eq!(metrics.total_searches, 2);
        assert_eq!(metrics.cache_hits, 1);
        assert_eq!(metrics.cache_misses, 1);
        assert!((metrics.cache_hit_rate() - 0.5).abs() < 0.001);
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_parallel_vector_search() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        for i in 0..1500 {
            let angle = (i as f32) * 0.01;
            retriever.add_vector(vec![angle.cos(), angle.sin(), 0.0]);
        }
        let query = vec![1.0, 0.0, 0.0];
        let results = retriever.vector_search(&query, 10);
        assert_eq!(results.len(), 10);
        assert_eq!(results[0].0, 0);
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_cache_clear() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.add_document("Test", "Rust programming").unwrap();
        let _ = retriever.search("Rust").unwrap();
        let (cache_size, _) = retriever.cache_stats();
        assert_eq!(cache_size, 1);
        retriever.clear_cache();
        let (cache_size, _) = retriever.cache_stats();
        assert_eq!(cache_size, 0);
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_metrics_index_size() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.add_document("Test", "Rust programming").unwrap();
        let metrics = retriever.get_metrics();
        let index_size = metrics.get_index_size_bytes().unwrap();
        assert!(index_size > 0);
        let human_size = metrics.get_index_size_human().unwrap();
        assert!(!human_size.is_empty());
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_metrics_reset() {
        let (mut retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.add_document("Test", "Rust programming").unwrap();
        let _ = retriever.search("Rust").unwrap();
        let original_metrics = retriever.get_metrics();
        assert!(original_metrics.total_searches > 0);
        retriever.reset_metrics();
        let reset_metrics = retriever.get_metrics();
        assert_eq!(reset_metrics.total_searches, 0);
        assert_eq!(reset_metrics.cache_hits, 0);
        assert_eq!(reset_metrics.cache_misses, 0);
        assert_eq!(reset_metrics.total_documents_indexed, 0);
        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_health_check() {
        let (retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        retriever.health_check().unwrap();

        let mut retriever = retriever;
        retriever.add_vector_with_id("doc1".to_string(), vec![1.0; 384]); 
        retriever.add_vector_with_id("doc2".to_string(), vec![2.0; 384]);
        retriever.health_check().unwrap();

        retriever.doc_id_to_vector_idx.insert("extra_doc".to_string(), 0);
        let result = retriever.health_check();
        assert!(result.is_err());
        if let Err(RetrieverError::VectorError(msg)) = result {
            assert!(msg.contains("Vector storage inconsistency"));
        } else {
            panic!("Expected VectorError for inconsistency");
        }

        retriever.doc_id_to_vector_idx.remove("extra_doc");
        retriever.health_check().unwrap();

        retriever.doc_id_to_vector_idx.insert("doc1".to_string(), 999);
        let result = retriever.health_check();
        assert!(result.is_err());
        if let Err(RetrieverError::VectorError(msg)) = result {
            assert!(msg.contains("Invalid vector index"));
        } else {
            panic!("Expected VectorError for invalid index");
        }

        retriever.doc_id_to_vector_idx.insert("doc1".to_string(), 0);
        retriever.health_check().unwrap();

        drop(retriever);
        cleanup_test_files(&test_dir, &vector_file);
    }

    #[test]
    fn test_health_check_with_corrupted_index_dir() {
        let (retriever, test_dir) = setup_test_retriever();
        let vector_file = retriever.vector_file_path.clone();
        let _ = fs::remove_dir_all(&test_dir);
        let result = retriever.health_check();
        assert!(result.is_err());
        if let Err(RetrieverError::DirectoryError(msg)) = result {
            assert!(msg.contains("does not exist"));
        } else {
            panic!("Expected DirectoryError");
        }
        drop(retriever);
        let _ = fs::remove_file(&vector_file);
    }
}